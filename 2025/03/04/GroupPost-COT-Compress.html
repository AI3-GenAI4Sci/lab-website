<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>GroupPost20240304-COT Compress | AI3-GenAI4Sci</title>

<link rel="icon" href="/lab-website/images/icon.png">

<meta name="title" content="GroupPost20240304-COT Compress">
<meta name="description" content="An engaging 1-3 sentence description of your lab.">

<meta property="og:title" content="GroupPost20240304-COT Compress">
<meta property="og:site_title" content="AI3-GenAI4Sci">
<meta property="og:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="og:url" content="https://ai3-genai4sci.github.io/lab-website">
<meta property="og:image" content="/lab-website/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="GroupPost20240304-COT Compress">
<meta property="twitter:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="twitter:url" content="https://ai3-genai4sci.github.io/lab-website">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/lab-website/images/share.jpg">


  <meta name="author" content='&lt;!-- excerpt start --&gt;

&lt;p&gt;&lt;strong&gt;Report Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This report explores Chain-of-Thought (COT) compression techniques aimed at reducing the computational cost and enhancing the efficiency of large language models (LLMs) in complex reasoning tasks. While COT improves LLM performance by generating intermediate reasoning steps, the length of these reasoning chains leads to increased computational demands. COT compression seeks to shorten these chains without significantly impacting model performance.&lt;/p&gt;

&lt;p&gt;&lt;!-- excerpt end --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key Strategies and Methods&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The report covers several key strategies for COT compression:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Explicit Compression During Training:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Knowledge Distillation:&lt;/strong&gt; This involves transferring knowledge from a complex “System 2” model (which generates COT) to a more efficient “System 1” model (which directly outputs results), thereby speeding up the inference process.
        &lt;ul&gt;
          &lt;li&gt;Notable work: Meta’s “Distilling System 2 into System 1.”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Step-by-Step Training:&lt;/strong&gt; This method iteratively trains models to identify and skip redundant reasoning steps, effectively shortening the reasoning pathway.
        &lt;ul&gt;
          &lt;li&gt;Notable work: Research by Qiu et al. in “Can Language Models Learn to Skip Steps?”.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Data-Conditioned Training:&lt;/strong&gt; Utilizes GPT-4 to create pairs of long and short COT data, training models to generate reasoning chains of varying lengths.
        &lt;ul&gt;
          &lt;li&gt;Notable work: Research by BeiKe, “C3OT: Generating Shorter Chain-of-Thought without Compromising Effectiveness.”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hidden State Compression:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Implicit COT:&lt;/strong&gt; Gradually internalizes the explicit COT reasoning process into the model’s hidden states, eliminating the need for explicit reasoning chains.
        &lt;ul&gt;
          &lt;li&gt;Notable work: Research by Yejin Choi et al., “From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step.”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Compressed COT (CCOT):&lt;/strong&gt; Employs densely semantic tokens to represent compressed reasoning, compressing reasoning chains within the hidden space.
        &lt;ul&gt;
          &lt;li&gt;Notable work: Research by JHU, “Compressed Chain of Thought: Efficient Reasoning Through Dense Representations.”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic Length Control:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;COT-Valve:&lt;/strong&gt; Introduces adjustable parameters within the model’s parameter space to dynamically manage the length of reasoning chains.
        &lt;ul&gt;
          &lt;li&gt;Notable work: Research by NUS, “COT-Valve: Length-Compressible Chain-of-Thought Tuning.”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reinforcement Learning Compression:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;O1-Pruner:&lt;/strong&gt; Designs a length-harmonizing reward function to guide models in generating shorter yet accurate reasoning sequences through reinforcement learning.
        &lt;ul&gt;
          &lt;li&gt;Notable work: Research by Sun Yat-sen University, “O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning.”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Kimi k1.5:&lt;/strong&gt; Explores various reinforcement learning compression strategies, including model merging, shortest rejection sampling, DPO, and Long2short RL.
        &lt;ul&gt;
          &lt;li&gt;Notable work: Kimi 1.5 Technical Report.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Key Research Trends&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A shift from explicit token compression to implicit representation compression, focusing on more efficient reasoning methods.&lt;/li&gt;
  &lt;li&gt;The growing prominence of reinforcement learning in reasoning chain compression.&lt;/li&gt;
  &lt;li&gt;The exploration of multi-agent frameworks for social reasoning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Future Outlook&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As models continue to scale, managing inference costs will become increasingly critical.&lt;/li&gt;
  &lt;li&gt;The development of more effective compression techniques is essential for deploying LLMs in resource-constrained environments.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Report Significance&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provides a comprehensive overview of COT compression technologies, highlighting the latest advancements in the field.&lt;/li&gt;
  &lt;li&gt;Offers practical insights for optimizing LLM performance and efficiency through appropriate compression strategies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The specific files can be found here: &lt;a href="https://github.com/AI3-GenAI4Sci/lab-website/blob/main/docs/TrendReport_ChainCompression-20250304.pdf"&gt;Trend Report.pdf&lt;/a&gt;&lt;/p&gt;
'>
  <meta property="og:type" content="article">
  <meta property="og:updated_time" content="2025-04-10T08:53:29+00:00">
  <meta property="article:published_time" content="2025-03-04T00:00:00+00:00">
  <meta property="article:modified_time" content="2025-04-10T08:53:29+00:00">
  <meta name="revised" content="2025-04-10T08:53:29+00:00">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "BlogPosting",
      "author": { "@type": "Person", "name": "GroupPost20240304-COT Compress" },
      "datePublished": "2025-03-04T00:00:00+00:00",
      "dateModified": "2025-04-10T08:53:29+00:00",
    
    "name": "GroupPost20240304-COT Compress",
    "description": "An engaging 1-3 sentence description of your lab.",
    "headline": "GroupPost20240304-COT Compress",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/lab-website/images/icon.png" }
    },
    "url": "https://ai3-genai4sci.github.io/lab-website"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="https://ai3-genai4sci.github.io/lab-website/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/lab-website/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/all.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/background.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/body.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/button.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/card.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/code.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/details.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/float.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/font.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/form.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/header.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/image.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/link.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/list.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/main.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/section.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/table.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/lab-website/_styles/util.css" rel="stylesheet">
  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/lab-website/_scripts/anchors.js"></script>

  <script src="/lab-website/_scripts/dark-mode.js"></script>

  <script src="/lab-website/_scripts/fetch-tags.js"></script>

  <script src="/lab-website/_scripts/search.js"></script>

  <script src="/lab-website/_scripts/site-search.js"></script>

  <script src="/lab-website/_scripts/table-wrap.js"></script>

  <script src="/lab-website/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/lab-website/lab-website/images/background.jpg')" data-dark="true">
  <a href="/lab-website/" class="home">
    
      <span class="logo">
        
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-40 -60 80 100">
  <style>
    .bubble {
      animation: float 2s ease-out both infinite var(--delay);
    }
    @keyframes float {
      0% {
        opacity: 0;
      }
      50% {
        transform: translateY(0);
        opacity: 0;
      }
      75% {
        opacity: 1;
      }
      100% {
        opacity: 0;
        transform: translateY(-40px);
      }
    }
  </style>
  <g fill="currentColor" opacity="0.5">
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.1s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.4s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 1.1s"></circle>
  </g>
  <path fill="#38bdf8" d="
      M 0 -22.5
      L -19.5 -11.25
      L -19.5 11.25
      L 0 22.5
      L 19.5 11.25
      L 19.5 -11.25
      z
    "></path>
  <path fill="#bae6fd" d="
      M 0 -22.5
      L -19.5 -11.25
      L 0 0
      L 19.5 -11.25
      z
    "></path>
  <path fill="none" stroke="currentColor" stroke-width="5" d="
      M -18 -53
      L -10 -53
      L -10 -29.2
      L -30.3 -17.5
      L -30.3 17.5
      L 0 35
      L 30.3 17.5
      L 30.3 -17.5
      L 10 -29.2
      L 10 -53
      L 18 -53
    "></path>
</svg>

        
      </span>
    
    
      <span class="title-text" data-tooltip="Home">
        
          <span class="title">AI3-GenAI4Sci</span>
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/lab-website/research/" data-tooltip="Published works">
          Research
        </a>
      
    
      
        <a href="/lab-website/projects/" data-tooltip="Software, datasets, and more">
          Projects
        </a>
      
    
      
        <a href="/lab-website/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/lab-website/blog/" data-tooltip="Musings and miscellany">
          Blog
        </a>
      
    
      
        <a href="/lab-website/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="1">
    <!--
  background: ;
  dark: ;
  size: 1;
-->


<h1 class="center">GroupPost20240304-COT Compress</h1>

<div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Rui Xu</span>
      </span>
    
  

  
  

  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>March 04, 2025</span>
    </span>
  

  
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span>April 10, 2025</span>
    </span>
  
</div>


  


  <div class="tags" data-link="/lab-website/blog">
    
      <a href="blog?search=%22tag:%20llm%22" class="tag" data-tooltip='Show items with the tag "llm"'>
        llm
      </a>
    
      <a href="blog?search=%22tag:%20reinforce-learning%22" class="tag" data-tooltip='Show items with the tag "reinforce-learning"'>
        reinforce-learning
      </a>
    
      <a href="blog?search=%22tag:%20cot%22" class="tag" data-tooltip='Show items with the tag "cot"'>
        cot
      </a>
    
  </div>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->


<!-- excerpt start -->

<p><strong>Report Overview</strong></p>

<p>This report explores Chain-of-Thought (COT) compression techniques aimed at reducing the computational cost and enhancing the efficiency of large language models (LLMs) in complex reasoning tasks. While COT improves LLM performance by generating intermediate reasoning steps, the length of these reasoning chains leads to increased computational demands. COT compression seeks to shorten these chains without significantly impacting model performance.</p>

<p><!-- excerpt end --></p>

<p><strong>Key Strategies and Methods</strong></p>

<p>The report covers several key strategies for COT compression:</p>

<ol>
  <li>
<strong>Explicit Compression During Training:</strong>
    <ul>
      <li>
<strong>Knowledge Distillation:</strong> This involves transferring knowledge from a complex “System 2” model (which generates COT) to a more efficient “System 1” model (which directly outputs results), thereby speeding up the inference process.
        <ul>
          <li>Notable work: Meta’s “Distilling System 2 into System 1.”</li>
        </ul>
      </li>
      <li>
<strong>Step-by-Step Training:</strong> This method iteratively trains models to identify and skip redundant reasoning steps, effectively shortening the reasoning pathway.
        <ul>
          <li>Notable work: Research by Qiu et al. in “Can Language Models Learn to Skip Steps?”.</li>
        </ul>
      </li>
      <li>
<strong>Data-Conditioned Training:</strong> Utilizes GPT-4 to create pairs of long and short COT data, training models to generate reasoning chains of varying lengths.
        <ul>
          <li>Notable work: Research by BeiKe, “C3OT: Generating Shorter Chain-of-Thought without Compromising Effectiveness.”</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Hidden State Compression:</strong>
    <ul>
      <li>
<strong>Implicit COT:</strong> Gradually internalizes the explicit COT reasoning process into the model’s hidden states, eliminating the need for explicit reasoning chains.
        <ul>
          <li>Notable work: Research by Yejin Choi et al., “From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step.”</li>
        </ul>
      </li>
      <li>
<strong>Compressed COT (CCOT):</strong> Employs densely semantic tokens to represent compressed reasoning, compressing reasoning chains within the hidden space.
        <ul>
          <li>Notable work: Research by JHU, “Compressed Chain of Thought: Efficient Reasoning Through Dense Representations.”</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Dynamic Length Control:</strong>
    <ul>
      <li>
<strong>COT-Valve:</strong> Introduces adjustable parameters within the model’s parameter space to dynamically manage the length of reasoning chains.
        <ul>
          <li>Notable work: Research by NUS, “COT-Valve: Length-Compressible Chain-of-Thought Tuning.”</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Reinforcement Learning Compression:</strong>
    <ul>
      <li>
<strong>O1-Pruner:</strong> Designs a length-harmonizing reward function to guide models in generating shorter yet accurate reasoning sequences through reinforcement learning.
        <ul>
          <li>Notable work: Research by Sun Yat-sen University, “O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning.”</li>
        </ul>
      </li>
      <li>
<strong>Kimi k1.5:</strong> Explores various reinforcement learning compression strategies, including model merging, shortest rejection sampling, DPO, and Long2short RL.
        <ul>
          <li>Notable work: Kimi 1.5 Technical Report.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Key Research Trends</strong></p>

<ul>
  <li>A shift from explicit token compression to implicit representation compression, focusing on more efficient reasoning methods.</li>
  <li>The growing prominence of reinforcement learning in reasoning chain compression.</li>
  <li>The exploration of multi-agent frameworks for social reasoning.</li>
</ul>

<p><strong>Future Outlook</strong></p>

<ul>
  <li>As models continue to scale, managing inference costs will become increasingly critical.</li>
  <li>The development of more effective compression techniques is essential for deploying LLMs in resource-constrained environments.</li>
</ul>

<p><strong>Report Significance</strong></p>

<ul>
  <li>Provides a comprehensive overview of COT compression technologies, highlighting the latest advancements in the field.</li>
  <li>Offers practical insights for optimizing LLM performance and efficiency through appropriate compression strategies.</li>
</ul>

<p>The specific files can be found here: <a href="https://github.com/AI3-GenAI4Sci/lab-website/blob/main/docs/TrendReport_ChainCompression-20250304.pdf">Trend Report.pdf</a></p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->


<div class="post-nav">
  <span>
    
      <i class="icon fa-solid fa-angle-left"></i> Previous post<br>
      <a href="/lab-website/2025/02/25/GroupPost-WorkAlign.html">
        GroupPost20250225-WorkAlign
      </a>
    
  </span>
  <span>
    
      Next post <i class="icon fa-solid fa-angle-right"></i><br>
      <a href="/lab-website/2025/03/25/GroupPost-WorkAlign.html">
        GroupPost20250325-WorkAlign
      </a>
    
  </span>
</div>
  </section>


    </main>
    


<footer class="background" style="--image: url('/lab-website/lab-website/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:contact@AI3-GenAI4Sci.com" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0001-8713-9213" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=ETJoidYAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/AI3-GenAI4Sci" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://twitter.com/AI3-GenAI4Sci" data-tooltip="Twitter" data-style="bare" aria-label="Twitter">
      <i class="icon fa-brands fa-twitter"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://youtube.com/AI3-GenAI4Sci" data-tooltip="YouTube" data-style="bare" aria-label="YouTube">
      <i class="icon fa-brands fa-youtube"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    AI3-GenAI4Sci
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
